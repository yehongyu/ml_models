{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 1.15.0\n",
      "/Users/aodandan/data/model/\n",
      "None\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /Users/aodandan/data/model/saved_model/1591186929/variables/variables\n",
      "predict_by_predictor-probabilities: [0.4165174  0.41652605]\n",
      "class_ids [[0]\n",
      " [0]]\n",
      "classes [[b'0']\n",
      " [b'0']]\n",
      "all_class_ids [[0 1]\n",
      " [0 1]]\n",
      "all_classes [[b'0' b'1']\n",
      " [b'0' b'1']]\n",
      "logistic [[0.41651735]\n",
      " [0.41652605]]\n",
      "probabilities [[0.5834826  0.4165174 ]\n",
      " [0.5834739  0.41652605]]\n",
      "logits [[-0.33708644]\n",
      " [-0.33705074]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tf version:\", tf.__version__)\n",
    "\n",
    "cur_ts = int(time.time())\n",
    "tf.app.flags.DEFINE_string(\"train_paths\", None, \"HDFS paths to input files.\")\n",
    "tf.app.flags.DEFINE_string(\"eval_paths\", \"/Users/aodandan/data/tfrecord/eval/part-*\", \"eval data path\")\n",
    "tf.app.flags.DEFINE_string(\"model_path\", \"/Users/aodandan/data/model/dnn_estimator/\", \"Where to write output files.\")\n",
    "tf.app.flags.DEFINE_string(\"last_model_path\", \"\", \"Model path for the previous run.\")\n",
    "tf.app.flags.DEFINE_integer(\"train_epochs\", 1, \"train epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 512, \"batch size\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-4, \"train learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 0.5, \"dropout\")\n",
    "tf.app.flags.DEFINE_float(\"clip_norm\", 10.0, \"clip norm\")\n",
    "tf.app.flags.DEFINE_integer(\"num_cols\", 242, \"num cols\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "##print(FLAGS.train_paths)\n",
    "print(FLAGS.model_path)\n",
    "print(FLAGS.train_paths)\n",
    "\n",
    "def build_feature_columns():\n",
    "    columns = []\n",
    "    for i in range(FLAGS.num_cols):\n",
    "        num_column = tf.feature_column.numeric_column(\"slot_%s\"%i)\n",
    "        columns.append(num_column)\n",
    "    return columns\n",
    "\n",
    "def build_model(FLAGS):\n",
    "    print(FLAGS.model_path)\n",
    "    print(FLAGS.last_model_path)\n",
    "    print(FLAGS.learning_rate)\n",
    "    print(FLAGS.clip_norm)\n",
    "    print(FLAGS.num_cols)\n",
    "    print(FLAGS.dropout)\n",
    "    checkpoint_dir = FLAGS.model_path\n",
    "    if FLAGS.last_model_path and not tf.train.latest_checkpoint(checkpoint_dir):\n",
    "        warmup_dir = FLAGS.last_model_path\n",
    "    else:\n",
    "        warmup_dir = None\n",
    "\n",
    "    my_optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, FLAGS.clip_norm)\n",
    "\n",
    "    #DNNClassifier: Loss is calculated by using softmax cross entropy.\n",
    "\n",
    "    print(\"start build model\")\n",
    "    model = tf.estimator.DNNClassifier(\n",
    "        feature_columns=build_feature_columns(),\n",
    "        hidden_units=[256, 64],\n",
    "        optimizer = my_optimizer,\n",
    "        n_classes = 2,\n",
    "        dropout=FLAGS.dropout,\n",
    "        config=tf.estimator.RunConfig(model_dir=checkpoint_dir),\n",
    "        warm_start_from=warmup_dir)\n",
    "    print(\"build model end\")\n",
    "    return model\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    features = {}\n",
    "    for i in range(FLAGS.num_cols):\n",
    "        fname = \"slot_%s\"%(i)\n",
    "        features[fname] = tf.placeholder(tf.float32, shape=[None], name=fname)\n",
    "    return tf.estimator.export.ServingInputReceiver(features, features)\n",
    "\n",
    "def read_data(paths, batch_size=512, num_epochs=1, shuffle=False, buffer_size=50000, num_cols=242, num_parallels=1, num_workers=1, worker_index=0):\n",
    "    def parse(value):\n",
    "        desc = {\n",
    "                'slot_%s'%i: tf.FixedLenFeature([1], tf.float32, default_value=0.0) for i in range(0, num_cols)\n",
    "            }\n",
    "        desc[\"label\"] = tf.FixedLenFeature([1], tf.int64, default_value=0)\n",
    "        example = tf.parse_single_example(value, desc)\n",
    "        label = example[\"label\"]\n",
    "        label = tf.cast(label,tf.int32)\n",
    "        del example[\"label\"]\n",
    "        return example, label\n",
    "\n",
    "    print('paths:', paths)\n",
    "    data_files = tf.data.Dataset.list_files(paths)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(data_files, num_parallel_reads=num_parallels)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    # dataset = dataset.shard(num_workers, worker_index)\n",
    "\n",
    "    return dataset.map(parse, num_parallel_calls=num_parallels) \\\n",
    "                  .repeat(num_epochs).batch(batch_size) \\\n",
    "                  .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def train_input_fn():\n",
    "    return read_data(FLAGS.train_paths,\n",
    "                     batch_size=FLAGS.batch_size,\n",
    "                     num_epochs=FLAGS.train_epochs,\n",
    "                     shuffle=True,\n",
    "                     num_cols=FLAGS.num_cols,\n",
    "                     num_parallels=1)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return read_data(FLAGS.eval_paths,\n",
    "                     batch_size=FLAGS.batch_size,\n",
    "                     num_cols=FLAGS.num_cols,\n",
    "                     num_parallels=1)\n",
    "\n",
    "def load_model_and_print_variable():\n",
    "    path = FLAGS.model_path\n",
    "    init_vars = tf.train.list_variables(path)\n",
    "    for name, shape in init_vars:\n",
    "        array = tf.train.load_variable(path, name)\n",
    "        print(name, shape)\n",
    " \n",
    "def find_variable():\n",
    "    # TODO: find the two variable:\n",
    "    model = build_model(FLAGS)\n",
    "    weight_name = 'dnn/logits/kernel:0' # dnn/logits/bias, dnn/head/beta1_power, dnn/hiddenlayer_0/bias\n",
    "    score_tensor_name='dnn/head/predictions/logistic:0'\n",
    "    label_tensor_name='IteratorGetNext:%s' % (FLAGS.num_cols)\n",
    "    \n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    score_tensor = graph.get_tensor_by_name(weight_name)\n",
    "    sess = tf.compat.v1.Session(graph=graph)\n",
    "    sess.run(model)\n",
    "    print(\"score_tensor:\", sess.run(score_tensor))\n",
    "\n",
    "def get_x_map():\n",
    "    x_map = {}\n",
    "    for i in range(FLAGS.num_cols):\n",
    "        fname = \"slot_%s\"%(i)\n",
    "        #x_map[fname] = np.arange(2)\n",
    "        x_map[fname] = [0, 1]\n",
    "    return x_map\n",
    "\n",
    "def start_train():\n",
    "    FLAGS.train_paths = [\"/Users/aodandan/data/tfrecord/train/part-*\", \"/Users/aodandan/data/tfrecord/eval/part-*\"]\n",
    "    #FLAGS.train_paths = \"/Users/aodandan/data/tfrecord/{train,eval}/part-*\"\n",
    "\n",
    "    model = build_model(FLAGS)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=6000000)\n",
    "    \n",
    "    feature_spec = tf.feature_column.make_parse_example_spec(build_feature_columns())\n",
    "    export_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
    "    exporter = tf.estimator.FinalExporter('gandalf', export_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=100, throttle_secs=10, exporters=[exporter])\n",
    "    \n",
    "    #print(\"estimator-variable names:\", tf.estimator.get_variable_names())\n",
    "    print(\"start to train\")\n",
    "    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n",
    "    \n",
    "    print(\"start to save model\")\n",
    "    model.export_saved_model(FLAGS.model_path + '/saved_model',\n",
    "                serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "    \n",
    "    x_map = get_x_map()\n",
    "    pred_input_fn = tf.estimator.inputs.numpy_input_fn(x=x_map, shuffle=False)\n",
    "    pred_results = model.predict(input_fn=pred_input_fn)\n",
    "    print(\"AA pred_results:\", type(pred_results))\n",
    "    for pred_dict in pred_results:\n",
    "        score = pred_dict['probabilities'][1]\n",
    "        print(\"AA pred_score:\", score)\n",
    "            \n",
    "    print(\"finish save model\")\n",
    "\n",
    "def predict_with_build_model():\n",
    "    FLAGS.model_path = \"/Users/aodandan/data/model\"\n",
    "    estimator = build_model(FLAGS)        \n",
    "\n",
    "    # predict with the model and print results\n",
    "    x_map = get_x_map()\n",
    "    pred_input_fn = tf.estimator.inputs.numpy_input_fn(x=x_map, shuffle=False)\n",
    "    \n",
    "    pred_results = estimator.predict(input_fn=pred_input_fn)\n",
    "    print(\"pred_results:\", type(pred_results))\n",
    "    for pred_dict in pred_results:\n",
    "        score = pred_dict['probabilities'][1]\n",
    "        print(\"pred_score:\", score)\n",
    "\n",
    "def predict_by_predictor():\n",
    "    model_path = \"/Users/aodandan/data/model/saved_model/1591186929\"\n",
    "    predict_fn = tf.contrib.predictor.from_saved_model(model_path, signature_def_key=\"predict\")\n",
    "    predictions = predict_fn(get_x_map())\n",
    "    print(\"predict_by_predictor-probabilities:\", np.asarray(predictions['probabilities'][:,1]))\n",
    "    for key in predictions:\n",
    "        print(key, predictions[key])\n",
    "    \n",
    "\n",
    "def simple_linear_train():\n",
    "    input_columns = []\n",
    "    input_columns.append(tf.feature_column.numeric_column(\"x\"))\n",
    "    input_columns.append(tf.feature_column.numeric_column(\"y\"))\n",
    "\n",
    "    estimator = tf.estimator.LinearClassifier(feature_columns=input_columns)\n",
    "\n",
    "    def input_fn():\n",
    "        return tf.data.Dataset.from_tensor_slices(\n",
    "            ({\"x\": [1., 2., 3., 4.], \"y\": [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(16)\n",
    "    estimator.train(input_fn)\n",
    "\n",
    "    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "    tf.feature_column.make_parse_example_spec(input_columns))\n",
    "    estimator_base_path = os.path.join(FLAGS.model_path, 'from_estimator')\n",
    "    estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)\n",
    "    return estimator_path\n",
    "\n",
    "def predict_by_load():\n",
    "    model_path = \"/Users/aodandan/data/model/saved_model/1591186929\"\n",
    "    #model = tf.compat.v2.saved_model.load(model_path)\n",
    "    model = tf.compat.v1.saved_model.load_v2(model_path)\n",
    "    print(list(model.signatures.keys()))\n",
    "    model_fn = model.signatures['predict']\n",
    "    print(model_fn.structured_outputs)\n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    for i in range(FLAGS.num_cols):\n",
    "        fname = \"slot_%s\"%(i)\n",
    "        example.features.feature[fname].float_list.value.extend([0.0])\n",
    "    #print(example)\n",
    "    predictions=model_fn(examples=tf.constant([example.SerializeToString()]))\n",
    "    print(predictions)\n",
    "\n",
    "def simple_linear_test():\n",
    "    estimator_path = \"/Users/aodandan/data/model/from_estimator/1591239059\"\n",
    "    print(\"estimator_path:\", estimator_path)\n",
    "    imported = tf.compat.v1.saved_model.load_v2(estimator_path)\n",
    "    print(list(imported.signatures.keys()))\n",
    "    example = tf.train.Example()\n",
    "    example.features.feature[\"x\"].float_list.value.extend([1.5])\n",
    "    example.features.feature[\"y\"].float_list.value.extend([1.5])\n",
    "    print(example)\n",
    "    result = imported.signatures[\"predict\"](\n",
    "            examples=tf.constant([example.SerializeToString()]))\n",
    "    print(result)\n",
    "    \n",
    "#start_train()           \n",
    "#predict_with_build_model()\n",
    "predict_by_predictor()\n",
    "#predict_by_load()\n",
    "#simple_linear_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
