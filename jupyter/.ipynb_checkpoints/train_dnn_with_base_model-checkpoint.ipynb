{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 1.15.0\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-232454c7dd4b>:130: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-232454c7dd4b>:130: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:root:model train_var: w1:0, <tf.Variable 'w1:0' shape=(264, 256) dtype=float32_ref>\n",
      "INFO:root:model train_var: b1:0, <tf.Variable 'b1:0' shape=(1, 256) dtype=float32_ref>\n",
      "INFO:root:model train_var: w2:0, <tf.Variable 'w2:0' shape=(256, 64) dtype=float32_ref>\n",
      "INFO:root:model train_var: b2:0, <tf.Variable 'b2:0' shape=(1, 64) dtype=float32_ref>\n",
      "INFO:root:model train_var: w3:0, <tf.Variable 'w3:0' shape=(64, 2) dtype=float32_ref>\n",
      "INFO:root:model train_var: b3:0, <tf.Variable 'b3:0' shape=(1, 2) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:root:Build iterator from file: /Users/aodandan/data/tfrecord/train/part-*\n",
      "INFO:root:Build iterator from file: /Users/aodandan/data/tfrecord/eval/part-00093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Init global variable: ['w1:0', 'b1:0', 'w2:0', 'b2:0', 'w3:0', 'b3:0', 'dnn_core/beta1_power:0', 'dnn_core/beta2_power:0', 'w1/Adam:0', 'w1/Adam_1:0', 'b1/Adam:0', 'b1/Adam_1:0', 'w2/Adam:0', 'w2/Adam_1:0', 'b2/Adam:0', 'b2/Adam_1:0', 'w3/Adam:0', 'w3/Adam_1:0', 'b3/Adam:0', 'b3/Adam_1:0']\n",
      "INFO:root:Init local variable: ['dnn_metric/loss_metric/total:0', 'dnn_metric/loss_metric/count:0', 'dnn_metric/auc_metric/true_positives:0', 'dnn_metric/auc_metric/false_negatives:0', 'dnn_metric/auc_metric/true_negatives:0', 'dnn_metric/auc_metric/false_positives:0', 'dnn_metric/acc_metric/total:0', 'dnn_metric/acc_metric/count:0']\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Epoch-0, step-100: batch_loss=106354.796875, loss=1431921.5, auc=0.5218698978424072, acc=0.5314062237739563\n",
      "INFO:root:Epoch-0, step-200: batch_loss=605439.5625, loss=1079142.75, auc=0.5257760882377625, acc=0.5389843583106995\n",
      "INFO:root:Epoch-0, step-300: batch_loss=138485.984375, loss=931317.625, auc=0.5276805758476257, acc=0.5422396063804626\n",
      "INFO:root:Epoch-0, step-400: batch_loss=10437.74609375, loss=841654.375, auc=0.534864068031311, acc=0.5475000143051147\n",
      "INFO:root:Epoch-0, step-500: batch_loss=183477.546875, loss=827203.75, auc=0.5363889336585999, acc=0.5497812628746033\n",
      "INFO:root:Epoch-0, step-600: batch_loss=691428.75, loss=819306.875, auc=0.5395700335502625, acc=0.5524739623069763\n",
      "INFO:root:Epoch-0, step-700: batch_loss=14592.357421875, loss=805397.5625, auc=0.5412797927856445, acc=0.5529018044471741\n",
      "INFO:root:Epoch-0, step-800: batch_loss=697877.9375, loss=772286.5625, auc=0.5449999570846558, acc=0.5563671588897705\n",
      "INFO:root:Epoch-0, step-900: batch_loss=660297.375, loss=736929.0625, auc=0.5475104451179504, acc=0.5592882037162781\n",
      "INFO:root:Epoch-0: consumed all examples.\n",
      "INFO:root:Store model to /Users/aodandan/data/model/dnn_simple/1591845378/ckpt/dnn-956\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Eval, step-100: batch_loss=229388.171875, loss=351076.1875, auc=0.5904454588890076, acc=0.6142187714576721, skauc=0.5748739189435521, skacc=0.61421875\n",
      "INFO:root:Eval, step-200: batch_loss=2.0359816551208496, loss=489177.03125, auc=0.5725085139274597, acc=0.6096875071525574, skauc=0.5498090271651053, skacc=0.6096875\n",
      "INFO:root:Eval: consumed all examples.\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Epoch-1, step-1000: batch_loss=1263974.875, loss=507788.125, auc=0.5482217669487, acc=0.5610795617103577\n",
      "INFO:root:Epoch-1, step-1100: batch_loss=392229.03125, loss=460553.21875, auc=0.5575525760650635, acc=0.5702040195465088\n",
      "INFO:root:Epoch-1, step-1200: batch_loss=48340.73046875, loss=386452.875, auc=0.5682221055030823, acc=0.5794057250022888\n",
      "INFO:root:Epoch-1, step-1300: batch_loss=29847.8203125, loss=343793.03125, auc=0.5708869099617004, acc=0.5817133188247681\n",
      "INFO:root:Epoch-1, step-1400: batch_loss=79727.6484375, loss=322435.53125, auc=0.5707169771194458, acc=0.5822775959968567\n",
      "INFO:root:Epoch-1, step-1500: batch_loss=64631.86328125, loss=308835.0, auc=0.5736489295959473, acc=0.5829216241836548\n",
      "INFO:root:Epoch-1, step-1600: batch_loss=98846.96875, loss=305750.53125, auc=0.5736511945724487, acc=0.5836811065673828\n",
      "INFO:root:Epoch-1, step-1700: batch_loss=312378.0625, loss=298924.65625, auc=0.575725793838501, acc=0.5858954787254333\n",
      "INFO:root:Epoch-1, step-1800: batch_loss=3794.348876953125, loss=292247.75, auc=0.5769426822662354, acc=0.5857709050178528\n",
      "INFO:root:Epoch-1, step-1900: batch_loss=32354.28125, loss=284806.3125, auc=0.5786910653114319, acc=0.587576150894165\n",
      "INFO:root:Epoch-1: consumed all examples.\n",
      "INFO:root:Store model to /Users/aodandan/data/model/dnn_simple/1591845378/ckpt/dnn-1912\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Eval, step-100: batch_loss=56539.44140625, loss=86421.0703125, auc=0.6309036016464233, acc=0.6526562571525574, skauc=0.6101258742970178, skacc=0.65265625\n",
      "INFO:root:Eval, step-200: batch_loss=0.4405316114425659, loss=120410.3828125, auc=0.6120316982269287, acc=0.6482812762260437, skauc=0.582243467810823, skacc=0.64828125\n",
      "INFO:root:Eval: consumed all examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Epoch-2, step-2000: batch_loss=656145.875, loss=211962.5625, auc=0.6036049723625183, acc=0.6058238744735718\n",
      "INFO:root:Epoch-2, step-2100: batch_loss=6896.80078125, loss=214453.640625, auc=0.5992593169212341, acc=0.6032247543334961\n",
      "INFO:root:Epoch-2, step-2200: batch_loss=55070.03515625, loss=211878.484375, auc=0.5945121049880981, acc=0.60205078125\n",
      "INFO:root:Epoch-2, step-2300: batch_loss=287003.75, loss=205691.984375, auc=0.5952275991439819, acc=0.6029719710350037\n",
      "INFO:root:Epoch-2, step-2400: batch_loss=32635.171875, loss=198646.5625, auc=0.5955495238304138, acc=0.6029393076896667\n",
      "INFO:root:Epoch-2, step-2500: batch_loss=109098.15625, loss=192992.0625, auc=0.5929737687110901, acc=0.6002869606018066\n",
      "INFO:root:Epoch-2, step-2600: batch_loss=204469.625, loss=187004.5625, auc=0.592073917388916, acc=0.5997002124786377\n",
      "INFO:root:Epoch-2, step-2700: batch_loss=184510.21875, loss=180355.09375, auc=0.5930889248847961, acc=0.6009676456451416\n",
      "INFO:root:Epoch-2, step-2800: batch_loss=72653.453125, loss=174500.625, auc=0.5951889157295227, acc=0.6026710271835327\n",
      "INFO:root:Epoch-2: consumed all examples.\n",
      "INFO:root:Store model to /Users/aodandan/data/model/dnn_simple/1591845378/ckpt/dnn-2868\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Eval, step-100: batch_loss=61471.5234375, loss=51041.80078125, auc=0.6183115243911743, acc=0.5745312571525574, skauc=0.6072456943403615, skacc=0.57453125\n",
      "INFO:root:Eval, step-200: batch_loss=860756.9375, loss=60360.32421875, auc=0.6125058531761169, acc=0.5649999976158142, skauc=0.6018302640622677, skacc=0.565\n",
      "INFO:root:Eval: consumed all examples.\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Epoch-3, step-2900: batch_loss=94015.28125, loss=100898.4296875, auc=0.6279792785644531, acc=0.62646484375\n",
      "INFO:root:Epoch-3, step-3000: batch_loss=120751.59375, loss=92058.3515625, auc=0.621965765953064, acc=0.62109375\n",
      "INFO:root:Epoch-3, step-3100: batch_loss=111053.359375, loss=90407.9453125, auc=0.6195987462997437, acc=0.618534505367279\n",
      "INFO:root:Epoch-3, step-3200: batch_loss=19905.9296875, loss=86995.2890625, auc=0.6115158796310425, acc=0.6125752925872803\n",
      "INFO:root:Epoch-3, step-3300: batch_loss=190032.625, loss=86794.6171875, auc=0.6087419390678406, acc=0.6102069020271301\n",
      "INFO:root:Epoch-3, step-3400: batch_loss=2807.41064453125, loss=89540.9921875, auc=0.6023339629173279, acc=0.6051456928253174\n",
      "INFO:root:Epoch-3, step-3500: batch_loss=105617.5078125, loss=87660.5390625, auc=0.5936754941940308, acc=0.5976067781448364\n",
      "INFO:root:Epoch-3, step-3600: batch_loss=48471.26953125, loss=89191.5625, auc=0.5937816500663757, acc=0.5994919538497925\n",
      "INFO:root:Epoch-3, step-3700: batch_loss=50659.4453125, loss=91374.2578125, auc=0.591650128364563, acc=0.5988957285881042\n",
      "INFO:root:Epoch-3, step-3800: batch_loss=5229.92724609375, loss=89928.15625, auc=0.5913814902305603, acc=0.5988130569458008\n",
      "INFO:root:Epoch-3: consumed all examples.\n",
      "INFO:root:Store model to /Users/aodandan/data/model/dnn_simple/1591845378/ckpt/dnn-3824\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Eval, step-100: batch_loss=74118.171875, loss=61682.5234375, auc=0.6148831844329834, acc=0.5743749737739563, skauc=0.6046028130253377, skacc=0.574375\n",
      "INFO:root:Eval, step-200: batch_loss=1038585.375, loss=72909.1484375, auc=0.6072190403938293, acc=0.5629687309265137, skauc=0.5978648245830682, skacc=0.56296875\n",
      "INFO:root:Eval: consumed all examples.\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Epoch-4, step-3900: batch_loss=121442.609375, loss=85523.234375, auc=0.5790203809738159, acc=0.5943667888641357\n",
      "INFO:root:Epoch-4, step-4000: batch_loss=58704.32421875, loss=90401.40625, auc=0.5876289010047913, acc=0.5980113744735718\n",
      "INFO:root:Epoch-4, step-4100: batch_loss=77977.53125, loss=82041.53125, auc=0.5887019634246826, acc=0.5992413759231567\n",
      "INFO:root:Epoch-4, step-4200: batch_loss=13217.666015625, loss=93715.9375, auc=0.5755914449691772, acc=0.5954122543334961\n",
      "INFO:root:Epoch-4, step-4300: batch_loss=40527.2734375, loss=89723.5390625, auc=0.5715029239654541, acc=0.5896139740943909\n",
      "INFO:root:Epoch-4, step-4400: batch_loss=117657.234375, loss=84106.4765625, auc=0.569690465927124, acc=0.5866970419883728\n",
      "INFO:root:Epoch-4, step-4500: batch_loss=89411.59375, loss=78887.421875, auc=0.5707938075065613, acc=0.5857987999916077\n",
      "INFO:root:Epoch-4, step-4600: batch_loss=18243.060546875, loss=74689.8359375, auc=0.570418655872345, acc=0.5842863917350769\n",
      "INFO:root:Epoch-4, step-4700: batch_loss=9197.0751953125, loss=70775.8359375, auc=0.5693638324737549, acc=0.5834938883781433\n",
      "INFO:root:Epoch-4: consumed all examples.\n",
      "INFO:root:Store model to /Users/aodandan/data/model/dnn_simple/1591845378/ckpt/dnn-4780\n",
      "INFO:root:run var: dnn_metric/loss_metric/total:0, <tf.Variable 'dnn_metric/loss_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/loss_metric/count:0, <tf.Variable 'dnn_metric/loss_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_positives:0, <tf.Variable 'dnn_metric/auc_metric/true_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_negatives:0, <tf.Variable 'dnn_metric/auc_metric/false_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/true_negatives:0, <tf.Variable 'dnn_metric/auc_metric/true_negatives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/auc_metric/false_positives:0, <tf.Variable 'dnn_metric/auc_metric/false_positives:0' shape=(200,) dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/total:0, <tf.Variable 'dnn_metric/acc_metric/total:0' shape=() dtype=float32_ref>\n",
      "INFO:root:run var: dnn_metric/acc_metric/count:0, <tf.Variable 'dnn_metric/acc_metric/count:0' shape=() dtype=float32_ref>\n",
      "INFO:root:Eval, step-100: batch_loss=1814.401123046875, loss=1490.9842529296875, auc=0.5913784503936768, acc=0.5517187714576721, skauc=0.5825305581998258, skacc=0.55171875\n",
      "INFO:root:Eval, step-200: batch_loss=25285.62890625, loss=1767.7730712890625, auc=0.5900880098342896, acc=0.5435937643051147, skauc=0.5803031144407754, skacc=0.54359375\n",
      "INFO:root:Eval: consumed all examples.\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "print(\"tf version:\", tf.__version__)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "cur_ts = str(int(time.time()))\n",
    "tf.app.flags.DEFINE_string(\"train_paths\", \"/Users/aodandan/data/tfrecord/train/part-*\", \"HDFS paths to input files.\")\n",
    "tf.app.flags.DEFINE_string(\"eval_paths\", \"/Users/aodandan/data/tfrecord/eval/part-00093\", \"eval data path\")\n",
    "tf.app.flags.DEFINE_string(\"model_path\", \"/Users/aodandan/data/model/dnn_simple/\"+cur_ts, \"Where to write output files.\")\n",
    "tf.app.flags.DEFINE_string(\"last_model_path\", \"\", \"Model path for the previous run.\")\n",
    "tf.app.flags.DEFINE_integer(\"train_epochs\", 5, \"train epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"batch size\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-3, \"train learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 0.5, \"dropout\")\n",
    "tf.app.flags.DEFINE_float(\"clip_norm\", 10.0, \"clip norm\")\n",
    "tf.app.flags.DEFINE_integer(\"num_cols\", 264, \"num cols\")\n",
    "tf.app.flags.DEFINE_string(\"f\", \"\", \"kernel\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "def cal_sklearn_auc(all_y, all_pred, batch_labels, batch_probs):\n",
    "    all_y.extend(batch_labels)\n",
    "    all_pred.extend(batch_probs)\n",
    "    auc = metrics.roc_auc_score(all_y, all_pred)\n",
    "    acc = metrics.accuracy_score(all_y, np.around(all_pred).astype(int))\n",
    "    return auc, acc\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, paths, shuffle, num_cols, batch_size):\n",
    "        self.iterator = self.build_iterator(paths, shuffle=shuffle, num_cols=num_cols, batch_size=batch_size)\n",
    "        self.initializer = self.iterator.initializer\n",
    "        self.next_element = self.iterator.get_next()\n",
    "        \n",
    "    def build_iterator(self, paths, shuffle=True, num_cols=264, batch_size=2, buffer_size = 8 * 1024 * 1024, num_parallels=1):\n",
    "        def parse(value):\n",
    "            desc = {\n",
    "                \"slot_%s\"%i: tf.io.FixedLenFeature([1], tf.float32, default_value=0.0) for i in range(0, num_cols)\n",
    "                }\n",
    "            desc[\"label\"] = tf.io.FixedLenFeature([1], tf.int64, default_value=0)\n",
    "            example = tf.io.parse_single_example(value, desc)\n",
    "            label = example[\"label\"]\n",
    "            label = tf.cast(label,tf.int32)\n",
    "            del example[\"label\"]\n",
    "            instance = []\n",
    "            for i in range(num_cols):\n",
    "                instance.append(example[\"slot_%s\"%i])\n",
    "            return instance, label\n",
    "    \n",
    "        logging.info(\"Build iterator from file: {}\".format(paths))\n",
    "        data_files = tf.data.Dataset.list_files(paths, shuffle=shuffle)\n",
    "        dataset = tf.data.TFRecordDataset(data_files, buffer_size=buffer_size, \n",
    "                                          num_parallel_reads=num_parallels)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "        dataset = dataset.map(parse, num_parallel_calls=num_parallels).batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=2 * batch_size)\n",
    "        return tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "\n",
    "class VerifyDNNModel(object):\n",
    "    def __init__(self, parameters):\n",
    "        self.lr = parameters.get(\"lr\", 0.001)\n",
    "        self.dropout = parameters.get(\"dropout\", 0.1)\n",
    "        self.n_feature = parameters.get(\"n_feature\", 264)\n",
    "        self.n_classes = parameters.get(\"n_classes\", 2)\n",
    "        self.device = parameters.get(\"device\", \"/cpu:0\")\n",
    "        \n",
    "        self.width_1th_layer = 256\n",
    "        self.width_2th_layer = 64\n",
    "        self.width_3th_layer = self.n_classes\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        \n",
    "        self.build_graph()\n",
    "                        \n",
    "        self.create_summary()\n",
    "    \n",
    "    def create_summary(self):\n",
    "        with tf.name_scope(\"dnn_metric\") as scope, tf.device(self.device):\n",
    "            loss_val, self.loss_op = tf.compat.v1.metrics.mean(self.loss, name=\"loss_metric\")\n",
    "            auc_val, self.auc_op = tf.compat.v1.metrics.auc(self.labels, self.prediction, name=\"auc_metric\")\n",
    "            pred_class = tf.cast(tf.round(self.prediction), tf.int32)\n",
    "            acc_val, self.acc_op = tf.compat.v1.metrics.accuracy(self.labels, pred_class, name=\"acc_metric\")\n",
    "            tf.summary.scalar('loss', loss_val)\n",
    "            tf.summary.scalar('auc', auc_val)\n",
    "            tf.summary.scalar('acc', acc_val)        \n",
    "            self.summary_merged = tf.summary.merge_all()\n",
    "        \n",
    "    def create_placeholders(self):\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, self.n_feature, 1), name='X')\n",
    "        self.Y = tf.placeholder(tf.int32, shape=(None, 1), name='Y')\n",
    "            \n",
    "    def build_graph(self):    \n",
    "        with tf.name_scope(\"dnn_core\") as scope, tf.device(self.device):\n",
    "            input_X = tf.reshape(tf.compat.v1.squeeze(self.X), [-1, self.n_feature])\n",
    "            onehot_Y = tf.reshape(tf.one_hot(tf.compat.v1.squeeze(self.Y), depth=self.n_classes), [-1, self.n_classes])\n",
    "            self.labels = onehot_Y[:,1]\n",
    "        \n",
    "            w1 = tf.get_variable(name=\"w1\", shape=[self.n_feature, self.width_1th_layer],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.get_variable(name=\"b1\", shape=[1, self.width_1th_layer], initializer=tf.compat.v1.zeros_initializer)\n",
    "        \n",
    "            w2 = tf.get_variable(name=\"w2\", shape=[self.width_1th_layer, self.width_2th_layer],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.get_variable(name=\"b2\", shape=[1, self.width_2th_layer], initializer=tf.compat.v1.zeros_initializer)\n",
    "        \n",
    "            w3 = tf.get_variable(name=\"w3\", shape=[self.width_2th_layer, self.width_3th_layer],\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3 = tf.get_variable(name=\"b3\", shape=[1, self.width_3th_layer], initializer=tf.compat.v1.zeros_initializer)\n",
    "        \n",
    "            z1 = tf.add(tf.matmul(input_X, w1), b1)\n",
    "            a1 = tf.nn.relu(z1)\n",
    "            #a1 = tf.nn.dropout(a1, rate=self.dropout)\n",
    "            z2 = tf.add(tf.matmul(a1, w2), b2)\n",
    "            a2 = tf.nn.relu(z2)\n",
    "            #a2 = tf.nn.dropout(a2, rate=self.dropout)\n",
    "            logits = tf.add(tf.matmul(a2, w3), b3)\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            self.prediction = probabilities[:,1]\n",
    "        \n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits,\n",
    "                        labels=onehot_Y\n",
    "                    )\n",
    "            self.loss_rmean = tf.reduce_mean(self.loss)\n",
    "        \n",
    "            train_vars = tf.trainable_variables()\n",
    "            for v in train_vars: logging.info(\"model train_var: {}, {}\".format(v.name, v))\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss_rmean, train_vars), clip_norm=5)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        \n",
    "    def get_train_task(self):\n",
    "        task_ops = {\n",
    "            \"train_op\": self.train_op,\n",
    "            \"loss_op\": self.loss_op,\n",
    "            \"auc_op\": self.auc_op,\n",
    "            \"acc_op\": self.acc_op,\n",
    "            \"summary\": self.summary_merged,\n",
    "            \"loss_rmean\": self.loss_rmean,\n",
    "        }\n",
    "        return task_ops\n",
    "    \n",
    "    def get_eval_task(self):\n",
    "        task_ops = {\n",
    "            \"loss_op\": self.loss_op,\n",
    "            \"auc_op\": self.auc_op,\n",
    "            \"acc_op\": self.acc_op,\n",
    "            \"loss_rmean\": self.loss_rmean,\n",
    "            \"prediction\": self.prediction,\n",
    "        }\n",
    "        return task_ops\n",
    "\n",
    "class DNNTrainer(object):\n",
    "    def __init__(self):\n",
    "        self.log_path = FLAGS.model_path + \"/log/\" # tensorboard -â€“logdir\n",
    "        self.checkpoint_path = FLAGS.model_path + \"/ckpt/dnn\"\n",
    "        self.last_checkpoint_path = FLAGS.last_model_path\n",
    "        self.train_epochs = FLAGS.train_epochs\n",
    "        \n",
    "        parameters = {}\n",
    "        parameters[\"lr\"] = FLAGS.learning_rate\n",
    "        parameters[\"dropout\"] = FLAGS.dropout\n",
    "        parameters[\"n_feature\"] = FLAGS.num_cols\n",
    "        self.dnn_model = VerifyDNNModel(parameters)\n",
    "        \n",
    "        self.train_iterator = DataIterator(paths=FLAGS.train_paths, num_cols=FLAGS.num_cols, \n",
    "                                                        batch_size=FLAGS.batch_size, shuffle=True)\n",
    "        self.eval_iterator = DataIterator(paths=FLAGS.eval_paths, num_cols=FLAGS.num_cols,\n",
    "                                                       batch_size=FLAGS.batch_size, shuffle=False)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.train_epochs)\n",
    "\n",
    "    def get_sess_config(self):\n",
    "        sess_config = tf.ConfigProto()\n",
    "        sess_config.log_device_placement = True # log device placement\n",
    "        sess_config.gpu_options.allow_growth = True # dynamic allocate mem\n",
    "        sess_config.allow_soft_placement = True # auto select device\n",
    "        return sess_config\n",
    "    \n",
    "    def reset_running_variables(self, sess, scope):\n",
    "        running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=scope)\n",
    "        for v in running_vars: logging.info(\"run var: {}, {}\".format(v.name, v))\n",
    "        running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "        sess.run(running_vars_initializer)\n",
    "\n",
    "    def train_one_epoch(self, sess, log_writer, epoch, step):\n",
    "        sess.run(self.train_iterator.initializer)\n",
    "        self.reset_running_variables(sess, \"dnn_metric\") # accumulative, reset for each epoch\n",
    "        next_element = self.train_iterator.next_element\n",
    "        task_ops = self.dnn_model.get_train_task()\n",
    "        while True:\n",
    "            try:\n",
    "                batch_instances, batch_labels = sess.run(next_element)\n",
    "                feed = {self.dnn_model.X:batch_instances, self.dnn_model.Y:batch_labels}\n",
    "                \n",
    "                results = sess.run(task_ops, feed_dict=feed)\n",
    "        \n",
    "                step += 1\n",
    "                log_writer.add_summary(results['summary'], step)\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    logging.info(\"Epoch-{}, step-{}: batch_loss={}, loss={}, auc={}, acc={}\".format(\n",
    "                        epoch, step, results[\"loss_rmean\"], results[\"loss_op\"], results[\"auc_op\"], results[\"acc_op\"])\n",
    "                     )\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                logging.info(\"Epoch-{}: consumed all examples.\".format(epoch))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                err_msg = traceback.format_exc()\n",
    "                looging.error(\"err: {}\".format(err_msg))\n",
    "                break\n",
    "        return step\n",
    "\n",
    "    def eval_one(self, sess, step=0):\n",
    "        sess.run(self.eval_iterator.initializer)\n",
    "        self.reset_running_variables(sess, \"dnn_metric\") # accumulative, reset for each epoch       \n",
    "        next_element = self.eval_iterator.next_element\n",
    "        task_ops = self.dnn_model.get_eval_task()\n",
    "        all_y, all_pred = [], []\n",
    "        while True:\n",
    "            try:\n",
    "                batch_instances, batch_labels = sess.run(next_element)\n",
    "                feed = {self.dnn_model.X:batch_instances, self.dnn_model.Y:batch_labels}\n",
    "                \n",
    "                results = sess.run(task_ops, feed_dict=feed)\n",
    "                auc, acc = cal_sklearn_auc(all_y, all_pred, batch_labels, results[\"prediction\"])\n",
    "        \n",
    "                step += 1\n",
    "                if step % 100 == 0:\n",
    "                    logging.info(\"Eval, step-{}: batch_loss={}, loss={}, auc={}, acc={}, skauc={}, skacc={}\".format(\n",
    "                        step, results[\"loss_rmean\"], results[\"loss_op\"], results[\"auc_op\"], results[\"acc_op\"], auc, acc)\n",
    "                     )\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                logging.info(\"Eval: consumed all examples.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                err_msg = traceback.format_exc()\n",
    "                looging.error(\"err: {}\".format(err_msg))\n",
    "                break\n",
    "        return step\n",
    "    \n",
    "    def train(self):\n",
    "        with tf.Session(config=self.get_sess_config()) as sess:\n",
    "            if self.last_checkpoint_path:\n",
    "                logging.info(\"Restore variable from file: {}\".format(self.last_checkpoint_path))\n",
    "                self.saver.restore(sess, self.last_checkpoint_path)\n",
    "            else:\n",
    "                logging.info(\"Init global variable: {}\".format([v.name for v in tf.global_variables()]))\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            logging.info(\"Init local variable: {}\".format([v.name for v in tf.local_variables()]))\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "        \n",
    "            log_writer = tf.summary.FileWriter(self.log_path, sess.graph)\n",
    "            step = 0\n",
    "            for epoch in range(self.train_epochs):\n",
    "                # train model\n",
    "                step = self.train_one_epoch(sess, log_writer, epoch, step)\n",
    "            \n",
    "                # save model\n",
    "                last_store_path = self.saver.save(sess, self.checkpoint_path, global_step=step)\n",
    "                logging.info(\"Store model to {}\".format(last_store_path))\n",
    "            \n",
    "                # eval model\n",
    "                self.eval_one(sess)\n",
    "                \n",
    "            log_writer.close()\n",
    "                \n",
    "trainer = DNNTrainer()\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
